使用两层神经元，固定每一层神经元之间的连接为300个。针对每层神经元的激活函数不同，做如下对比：
每一种组合训练5次，使用t10k_test_demo进行测试，取五次的平均值作为测量指标。

sigm & sigm:
15
97.99%
17
98.06%
16
97.93%

relu & relu:
17
97.78%
15
97.85%
16
97.88%

sigm & relu:
15
97.90%
15
97.88%
15
97.94%

由于仅仅是修改了激活函数的类型，相当于是常数级别的操作，因此对训练时间几乎无影响。

从浅层神经网络模型和深度学习模型考虑：
为什么Sigmoid和ReLu两种激活函数在深度学习模型中效果拔群，
而在浅层神经网络模型（如ELM）中效果没有非常突出。

(Sigmoid和ReLu的对比)
https://www.cnblogs.com/alexanderkun/p/5701694.html
